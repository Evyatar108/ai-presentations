{
  "demoId": "highlights-deep-dive",
  "version": "1.0",
  "lastModified": "2026-02-20T10:00:00.000Z",
  "instruct": "Speak in a clear, confident, professional tone at a moderate pace, suitable for a technical presentation to software engineers.",
  "slides": [
    {
      "chapter": 0,
      "slide": 1,
      "title": "Title",
      "instruct": "Speak with calm authority and a hint of intrigue, like opening a keynote.",
      "segments": [
        {
          "id": "main",
          "narrationText": "From four calls to one. This is the story of how we redesigned the Meeting Highlights prompt — collapsing a four-call GPT-4 pipeline into a single unified prompt, and cutting GPU costs by roughly seventy percent.",
          "visualDescription": "Title slide: 'From 4 Calls to 1: Redesigning the Meeting Highlights Prompt'",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 1,
      "slide": 1,
      "title": "Product Context",
      "segments": [
        {
          "id": "title",
          "narrationText": "Let's start with what Meeting Highlights actually produces.",
          "visualDescription": "Title: 'Meeting Highlights: AI Video Recaps'",
          "notes": ""
        },
        {
          "id": "pipeline",
          "narrationText": "It's a three-stage pipeline. A meeting transcript goes into an LLM, which produces structured editing metadata. That metadata is then consumed by a video assembly service that builds the final recap video. The model doesn't produce the video — it produces the editing instructions.",
          "visualDescription": "Three-box pipeline diagram: Transcript → LLM → Video Assembly",
          "notes": ""
        },
        {
          "id": "types",
          "narrationText": "The LLM produces two types of content. Abstractive narration — AI-written topic summaries spoken over muted meeting video. And extractive clips — the actual best moments from the meeting, with original audio and video preserved.",
          "visualDescription": "Two cards: Abstractive Narration and Extractive Clips",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 1,
      "slide": 2,
      "title": "COGS Problem",
      "instruct": "Speak with urgency and concern, emphasizing the severity of the cost problem.",
      "segments": [
        {
          "id": "metrics",
          "narrationText": "Here's the problem we faced. The V1 implementation used four sequential LLM calls per meeting. At projected GA scale, that would require roughly six hundred A100 GPUs — far exceeding approved quotas. This made Meeting Highlights a capacity blocker, preventing the GA rollout.",
          "visualDescription": "Three metric tiles: 4 LLM Calls, ~600 GPUs, Capacity Blocker",
          "notes": ""
        },
        {
          "id": "quote",
          "narrationText": "As our engineering manager put it — we needed a fundamentally different strategy. This wasn't a problem we could solve by adding more hardware.",
          "visualDescription": "Blockquote card with Eli Lekhtser quote",
          "notes": ""
        },
        {
          "id": "emphasis",
          "instruct": "Speak with firm conviction, slower pace, emphasizing every word.",
          "narrationText": "The cost was inherent in the prompt architecture itself. The fix had to come from prompt engineering.",
          "visualDescription": "Red emphasis box: 'The fix had to come from prompt engineering'",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 2,
      "slide": 1,
      "title": "Four-Call Pipeline",
      "segments": [
        {
          "id": "title",
          "narrationText": "Let's walk through the V1 architecture — the four-call pipeline.",
          "visualDescription": "Title: 'The Four-Call Pipeline'",
          "notes": ""
        },
        {
          "id": "call1",
          "narrationText": "Call one: abstractive generation. This identifies the key meeting topics and writes narration summaries for each one.",
          "visualDescription": "First pipeline box appears: highlights_abstractives",
          "notes": ""
        },
        {
          "id": "call2",
          "narrationText": "Call two: extractive selection. This selects the best verbatim clips from a pre-enumerated set of candidate ranges. We'll come back to how those candidates are generated — it's the critical cost driver.",
          "visualDescription": "Second pipeline box appears: highlights_extractives",
          "notes": ""
        },
        {
          "id": "call3",
          "narrationText": "Call three: extractive ranking. This ranks the selected clips by quality — interest level, clarity, and self-containment.",
          "visualDescription": "Third pipeline box appears: highlights_extractive_ranking",
          "notes": ""
        },
        {
          "id": "call4",
          "narrationText": "Call four: final assembly. This merges everything into a unified narrative, interleaving abstractive summaries with extractive clips in coherent order.",
          "visualDescription": "Fourth pipeline box appears: highlights_final",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 2,
      "slide": 2,
      "title": "Call Detail",
      "segments": [
        {
          "id": "code",
          "narrationText": "Here's what a single call looks like in the V1 code. This is the abstractives query — a prompt template embedded in a Python string. Notice the structure: system instructions telling the model to envision itself as a video editor, then detailed prose instructions for the task.",
          "visualDescription": "CodeBlock showing Python snippet from HighlightsPromptMaper.py",
          "notes": ""
        },
        {
          "id": "annotations",
          "narrationText": "Two things to notice. The instructions are prose paragraphs — vague natural language the model interprets freely. And each call chains to the next via parsed markdown tables, creating fragile dependencies.",
          "visualDescription": "Three annotation callouts: RAI rules, Prose instructions, Markdown table output",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 3,
      "slide": 1,
      "title": "Four Cost Drivers",
      "segments": [
        {
          "id": "title",
          "narrationText": "When we analyzed the V1 pipeline, we identified four structural cost drivers.",
          "visualDescription": "Title: 'Four Structural Cost Drivers'",
          "notes": ""
        },
        {
          "id": "driver1",
          "narrationText": "Driver one: four sequential calls means four times the compute cost — each consuming its own GPU allocation.",
          "visualDescription": "Card 1: 4 sequential calls = 4x compute cost",
          "notes": ""
        },
        {
          "id": "driver2",
          "narrationText": "Driver two: the transcript is sent as verbose JSON with keys like Index, Speaker, Start, End, and Utterance repeated for every single utterance — often five hundred or more times per meeting.",
          "visualDescription": "Card 2: Verbose JSON input",
          "notes": ""
        },
        {
          "id": "driver3",
          "narrationText": "Driver three: fragile markdown parsing between calls. One malformed table breaks the whole chain.",
          "visualDescription": "Card 3: Fragile markdown table parsing between calls",
          "notes": ""
        },
        {
          "id": "driver4",
          "instruct": "Speak with dramatic emphasis, pausing before revealing the biggest driver.",
          "narrationText": "And driver four — the biggest one. A combinatorial candidate explosion that generates O of n-squared candidate ranges. This was the single largest cost driver, and the one that required the most creative solution.",
          "visualDescription": "Card 4 highlighted in red: Combinatorial candidate explosion O(n²)",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 3,
      "slide": 2,
      "title": "Verbose JSON",
      "segments": [
        {
          "id": "comparison",
          "narrationText": "V1 wraps each utterance in a five-key JSON object. Five hundred utterances means twenty-five hundred wasted key tokens — before any actual content.",
          "visualDescription": "BeforeAfterSplit — Left: V1 verbose JSON. Right: token counter",
          "notes": ""
        },
        {
          "id": "caption",
          "narrationText": "A cost lever we'll address in V2.",
          "visualDescription": "Caption: 'Thousands of tokens spent on structural noise'",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 4,
      "slide": 1,
      "title": "Nested Loop",
      "segments": [
        {
          "id": "title",
          "narrationText": "Now for the biggest cost driver: the combinatorial candidate explosion.",
          "visualDescription": "Title: 'Combinatorial Candidate Explosion'",
          "notes": ""
        },
        {
          "id": "code",
          "narrationText": "Look at the two highlighted for-loops. For each topic, the outer loop iterates over every start utterance, the inner loop over every end after that start.",
          "visualDescription": "CodeBlock with nested for-loops highlighted in amber",
          "notes": ""
        },
        {
          "id": "annotation",
          "narrationText": "O of n-squared candidates per topic — each with the full duplicated text. This is where the token budget explodes.",
          "visualDescription": "Annotations: O(n) starts × O(n) ends = O(n²) candidates, full text duplicated",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 4,
      "slide": 2,
      "title": "O(n^2) Visualized",
      "instruct": "Speak with building intensity, like revealing the scale of a problem.",
      "segments": [
        {
          "id": "grid",
          "narrationText": "Each lit cell is one candidate range. Watch how quickly they multiply.",
          "visualDescription": "CandidateGrid: animated upper-triangle grid lighting up progressively",
          "notes": ""
        },
        {
          "id": "math",
          "narrationText": "At real scale: thirty utterances per topic gives four hundred thirty-five candidates. Multiply across five topics — over two thousand rows.",
          "visualDescription": "Scale-up math: 30 utterances → 435 candidates, 5 topics → 2000+ rows",
          "notes": ""
        },
        {
          "id": "context_window",
          "narrationText": "The V1 code then greedily packs these candidates into the prompt until it fills the entire 128K context window. This is why a single extractives call could consume more tokens than all other calls combined.",
          "visualDescription": "Animated bar filling to 128K tokens",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 5,
      "slide": 1,
      "title": "Format Comparison",
      "segments": [
        {
          "id": "title",
          "narrationText": "Now let's look at the V2 solution, starting with the first innovation: the compact transcript table.",
          "visualDescription": "Title: 'V2 Innovation: Compact Transcript Table'",
          "notes": ""
        },
        {
          "id": "split",
          "narrationText": "V1's verbose format on the left. On the right, V2: turn markers group utterances by speaker, pipe-delimited columns replace JSON keys, and local IDs restart within each turn.",
          "visualDescription": "BeforeAfterSplit — V1 JSON vs V2 compact table",
          "notes": ""
        },
        {
          "id": "table",
          "narrationText": "Speaker info moves to a turn tag header. Timestamps are omitted — pre-computed downstream. And the third column, max_end_utterance_id, encodes topic boundaries directly in the data.",
          "visualDescription": "Comparison table: what changed across 5 aspects",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 5,
      "slide": 2,
      "title": "max_end_utterance_id",
      "segments": [
        {
          "id": "row",
          "narrationText": "This is the key insight. Look at this single row: u2, the utterance text, and u4 as the max_end_utterance_id. That third column tells the model: if you start a clip at u2, the furthest you can extend it is u4. This single integer replaces the entire O of n-squared candidate enumeration.",
          "visualDescription": "Highlighted row with annotated arrows pointing to each column",
          "notes": ""
        },
        {
          "id": "visual",
          "narrationText": "V1 fans out hundreds of candidate rows from each start. V2 encodes the same constraint as a single row with a ceiling boundary.",
          "visualDescription": "Split visual — V1 many rows fanning out vs V2 single row with boundary",
          "notes": ""
        },
        {
          "id": "comparison",
          "narrationText": "The result: from filling the 128K context window to roughly five to ten thousand tokens. Quadratic to linear.",
          "visualDescription": "Comparison table: Representation, Token budget, Complexity — V1 vs V2",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 6,
      "slide": 1,
      "title": "Pseudocode Algorithm",
      "segments": [
        {
          "id": "title",
          "narrationText": "The second innovation: replacing prose instructions with a pseudocode algorithm. The principle is simple — prose invites creative interpretation. Pseudocode demands systematic execution.",
          "visualDescription": "Title + quote: 'Prose = creative interpretation. Pseudocode = systematic execution.'",
          "notes": ""
        },
        {
          "id": "code",
          "narrationText": "Here's the main function from the V2 prompt. It reads like executable code — parse turn markers, skip intro and closing, segment into topics, write narration, enumerate extractive candidates, filter, rank, and build the final narrative.",
          "visualDescription": "Large CodeBlock showing generate_highlights() pseudocode",
          "notes": ""
        },
        {
          "id": "outputs",
          "narrationText": "All six output fields in a single response: topics, order, ranges, ranking, narrative, and self-checks.",
          "visualDescription": "Six output fields listed with checkmarks",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 6,
      "slide": 2,
      "title": "Prose vs Pseudocode",
      "segments": [
        {
          "id": "comparison",
          "narrationText": "On the left, V1's prose — vague guidance about coverage and distinctness. On the right, V2's pseudocode — explicit function calls with constraints and named variables.",
          "visualDescription": "BeforeAfterSplit — V1 prose paragraph vs V2 pseudocode function calls",
          "notes": ""
        },
        {
          "id": "benefits",
          "narrationText": "Four benefits. Unambiguous execution order. Named variables as shared state. More precision than prose. And a single source of truth — one prompt, not four.",
          "visualDescription": "Four benefit cards with checkmarks",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 7,
      "slide": 1,
      "title": "Copy-then-Parse",
      "segments": [
        {
          "id": "title",
          "narrationText": "The third innovation: copy-then-parse — a chain-of-thought grounding technique that prevents the model from hallucinating IDs and references.",
          "visualDescription": "Title: 'Copy-then-Parse: Chain-of-Thought Grounding'",
          "notes": ""
        },
        {
          "id": "copy",
          "narrationText": "Step one: copy raw strings verbatim from the input — the turn opening tag, the pipe-delimited row. Literal copies, not interpretations.",
          "visualDescription": "CodeBlock Step 1: model copies raw strings from input",
          "notes": ""
        },
        {
          "id": "parse",
          "narrationText": "Step two: parse structured values from those copies. Speaker name from the turn tag, turn ID by stripping the prefix, utterance ID the same way. If any parsed value doesn't match its source string, we know something went wrong.",
          "visualDescription": "CodeBlock Step 2: model parses from copies with arrows showing derivation",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 7,
      "slide": 2,
      "title": "Self-Checks",
      "segments": [
        {
          "id": "title",
          "narrationText": "The V2 prompt includes a self-checks section — ten boolean validators the model runs against its own output. They catch errors at generation time, and in production they show you which constraints fail most often.",
          "visualDescription": "Title: 'Self-Checks: Model Validates Its Own Output'",
          "notes": ""
        },
        {
          "id": "grid",
          "narrationText": "Ten checks covering structural correctness — topic boundaries, ID uniqueness, extractive constraints, narrative alignment, and more. Any failure triggers an automatic retry.",
          "visualDescription": "Grid of 10 boolean check cards with cascade animation",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 8,
      "slide": 1,
      "title": "Results Metrics",
      "instruct": "Speak with energy and pride, celebrating the achievement.",
      "segments": [
        {
          "id": "title",
          "narrationText": "So what were the results?",
          "visualDescription": "Title: 'Results' with gradient text",
          "notes": ""
        },
        {
          "id": "calls",
          "narrationText": "Seventy-five percent call reduction — four down to one.",
          "visualDescription": "Metric card: 75% LLM Call Reduction (4 → 1)",
          "notes": ""
        },
        {
          "id": "tokens",
          "narrationText": "Sixty percent token reduction from the compact format and unified prompt.",
          "visualDescription": "Metric card: 60% Token Reduction",
          "notes": ""
        },
        {
          "id": "gpus",
          "instruct": "Speak with triumph, this is the headline number.",
          "narrationText": "And roughly seventy percent GPU reduction — six hundred A100s down to about one hundred eighty. The capacity blocker was gone.",
          "visualDescription": "Metric card: ~70% GPU Reduction (~600 → ~180 A100s)",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 8,
      "slide": 2,
      "title": "Quality and Impact",
      "segments": [
        {
          "id": "quality",
          "narrationText": "Quality held strong. No grounding regression. Seventy-five to eighty percent coverage. And in blind A-B tests, reviewers preferred V2.",
          "visualDescription": "Three quality tiles: Grounding, Coverage, Reviewers",
          "notes": ""
        },
        {
          "id": "roadmap",
          "narrationText": "This cost reduction directly unblocked the product roadmap. The feature entered private preview and is now on track for GA rollout.",
          "visualDescription": "Roadmap arrow: Cost Reduction → Private Preview → GA Rollout",
          "notes": ""
        },
        {
          "id": "quote",
          "instruct": "Speak as if quoting someone admiringly, slightly slower and more measured.",
          "narrationText": "In the words of our engineering manager: V2 is a compact prompt with only one LLM request that combines abstractive and extractive highlights generation into a single unified pipeline.",
          "visualDescription": "Eli Lekhtser quote card",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 9,
      "slide": 1,
      "title": "Five Lessons",
      "instruct": "Speak in a thoughtful, advisory tone, like sharing hard-won wisdom.",
      "segments": [
        {
          "id": "title",
          "narrationText": "Let me close with five lessons you can apply to your next LLM pipeline.",
          "visualDescription": "Title: 'Five Lessons for Your Next LLM Pipeline'",
          "notes": ""
        },
        {
          "id": "lesson1",
          "narrationText": "Lesson one: challenge the multi-call assumption. Multiple steps don't require multiple calls.",
          "visualDescription": "Card 1: Challenge the multi-call assumption",
          "notes": ""
        },
        {
          "id": "lesson2",
          "narrationText": "Lesson two: input format is a cost lever. The compact table cut tokens before we changed a single instruction.",
          "visualDescription": "Card 2: Input format is a cost lever",
          "notes": ""
        },
        {
          "id": "lesson3",
          "narrationText": "Lesson three: pseudocode beats prose. When you need the model to follow a specific algorithm, write it as pseudocode. It reduces ambiguity and makes the output more predictable.",
          "visualDescription": "Card 3: Pseudocode beats prose",
          "notes": ""
        },
        {
          "id": "lesson4",
          "narrationText": "Lesson four: force the model to ground itself. Copy first, parse second — it creates a verifiable chain.",
          "visualDescription": "Card 4: Force the model to ground itself",
          "notes": ""
        },
        {
          "id": "lesson5",
          "narrationText": "And lesson five: self-checks are both a safety net and a diagnostic signal. They catch failures at generation time and show you which constraints break most in production.",
          "visualDescription": "Card 5: Self-checks enable automation",
          "notes": ""
        }
      ]
    },
    {
      "chapter": 9,
      "slide": 2,
      "title": "Closing",
      "instruct": "Speak warmly and inspirationally.",
      "segments": [
        {
          "id": "thankyou",
          "narrationText": "Thank you for your time.",
          "visualDescription": "Large 'Thank You' with gradient animation",
          "notes": ""
        },
        {
          "id": "cta",
          "narrationText": "Try these techniques in your own pipelines. Prompt engineering scales — and the ROI can be measured in hundreds of GPUs.",
          "visualDescription": "CTA badge: 'Try the techniques — prompt engineering scales'",
          "notes": ""
        }
      ]
    }
  ]
}
