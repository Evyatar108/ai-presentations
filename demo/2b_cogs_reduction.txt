19
00:02:16,000 --> 00:02:22,000
Cost efficiency while maintaining quality is critical for scaling Meeting Highlights globally.

20
00:02:22,400 --> 00:02:28,800
The initial implementation required four sequential LLM calls per meeting, creating significant computational costs.

21
00:02:29,200 --> 00:02:36,965
The first prompt segmented transcripts into one to seven key topics.

22
00:02:36,800 --> 00:02:45,976
The second prompt extracted up to ten self-contained utterance blocks capturing key feedback, news, or demonstrations.

23
00:02:44,400 --> 00:02:50,400
The third prompt ranked utterance blocks by quality—clarity, intelligibility, self-containment, and interest.

24
00:02:51,600 --> 00:02:58,600
The fourth prompt synthesized a cohesive narrative with smooth transitions between abstractive and extractive sections.

25
00:03:00,000 --> 00:03:14,824
We collapsed the four-step pipeline into one unified prompt with leaner input. Dedicated tuning preserved the original algorithm as an internal reasoning chain: segment → extract → rank → compose.

26
00:03:08,000 --> 00:03:19,294
The model executes this sequence in a single invocation instead of four external calls.

27
00:03:15,600 --> 00:03:30,776
We trimmed input size significantly for lower cost. Separate tuning adapted the model to the slimmer schema and token budget while maintaining valid output for video generation.

28
00:03:23,200 --> 00:03:30,612
The impact: seventy-five percent fewer model invocations per meeting.

29
00:03:30,400 --> 00:03:40,635
GPU requirements dropped from six hundred to two hundred for global availability.

30
00:03:38,800 --> 00:03:44,800
This cut COGS by over fifty percent, enabling economically viable worldwide rollout.

31
00:03:46,400 --> 00:03:54,164
The unified prompt produces higher-quality highlights; internal reviewers strongly prefer it over the multi-prompt version.

32
00:03:54,000 --> 00:04:01,765
These improvements unblock private preview and enable general availability within capacity constraints.